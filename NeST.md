# NeST: NN synthesis tool
一个基于数据自动生成紧凑网络的方法，特别是在大数据集上。
### time: 2018/02/05

## Introduction
1. NeST 从一个种子网络开始，通过基于梯度的增长和基于尺度的对于神经元与连接进行剪枝，来迭代的调整网络结构。最终生成一个紧凑的网络结构。（手工设计的网络往往存在很多的冗余参数）
2. 传统的网络生成方法遍历结构的参数并且训练相应的网络结构直到其分类的收益递减点。（guess：传统方法对于一个极其冗余的网络进行剪枝，直到该任务的收益递减点？是一种剪切算法而不是生成算法。）这种方法有以下的缺陷:

    * 结构固定：多数BP模型训练参数而不是结构。
    * 时间冗长：通过试错的方式找到合适的网络结构十分低效，特别是针对大数据集。
    * 大量冗余：多数网络存在参数冗余的情况。
3. NeST根据人脑发育的过程同时训练参数和结构，来解决传统方式的缺陷。

    * 从一个种子网络开始。
    * 基于梯度信息（gradient information - baby brain)来增加神经元以及连接。这使得生成的网络可以适用于不同的任务。
    * 基于尺度信息（magnitude information - adult brain)来减少神经元以及连接。这使得生成的网络不会冗余。

## Traditional method
1. 传统的自动生成网络结构策略可以分为两种：
> 进化算法（EA，evolutionary algorithm）
>
* 一种在离散复杂的结构空间中进行搜索的方式
* 十分低效所以不适合大数据集
* pathNet采用模块化以及每一层sum的方式有效的减少了遗传算法的搜索空间，因此可以应用到更大的数据集上，ex：imageNet

> 结构适应型算法（SA，structure adaptation）
>
* 利用网络的状态特性（ex：权重的分布情况）来调整网络结构。
* 可以分为：  
    > constructive approach
    >
    * 对于深度大型神经网络十分低效
    * 存在不可忽视的网络冗余情况
    > destructive approach
    >
    * 依赖于一个大型的训练好的网络作为裁剪的起始状态
    * 然而训练一个大型网络到收敛是一个十分耗时的是错过程
2. SA到现在为止发展的并不是很成熟。

## 合成方法（Synthesis Methodology）
* NeST结合了SA中的construction和destruction方法
### Pipeline
1. 初始拥有一个种子网络，拥有部分稀疏的连接
1. 设置最大网络尺寸S和期望的准确率A
2. s = sizeof(Net), a = test(Net), train(Net)
3. Construction

    if s <= S and a < A :

    迭代地 [增加神经元、连接、特征图并训练], 直到s >= S or a >= A
4. Destruction

    迭代地 [剪枝并训练], 直到a < A

### Construction ( Gradient-based Growth )
> 增加连接
* 增加可以快速减小损失函数L的值的连接
* 对于所有的未激活的神经元连接，计算L关于w的偏导（可以基于整个数据集或者一大批数据集）。然后，激活其中绝对值最大的连接。（关于为什么要取绝对值：因为L关于w的偏导作用于w，可正可负，代表下降最快的时候w应当向哪个方向变化）
* 优点及其正确性：
    > 有利于避免网络陷入局部最小值

    > 在LeNet on MNIST 上的实验发现，图像中心的连接增长密度远大于图像的边界增长密度。这与手写数字几乎都在图像的中心这一点相符合。

    > 从神经科学的角度，这个算法与著名的Hebbian理论（Neurons that ﬁre together wire together ）在数学上一致。推导可以参考page5以及BP算法的推导。

> 增加神经元
* pipeline
    ![alt tag](https://github.com/Wind-Wing/readme_images/blob/master/NeST_Algo2.png)
* 对于新增加的神经元的连接强度采用G的平方根是为了模拟一个批量梯度下降的过程，使得损失函数L较小。（推导见paper公式1,2,3; 正确性验证见公式4,5,6）
* 在权重初始化完成后，采用a并根据l-1与l+1层的权重情况来增强l层建立的权重。
* 增强权重的过程类似于人脑的反复练习来强化连接的过程，同时也是为了防止在后一阶段中的剪枝过程中，连接由于太弱而被裁剪。
* 在实验中，发现a>0.3比较合适。
* 没有弄明白的地方：在pipeline中，why rand{1,-1}，why sgn（Gm，n）only for W in?

> 增加特征图
* 对于卷积层：
    > 神经元的连接的增加与上述相同。
    
    > 卷积核的增加采用随机产生多个卷积核后，选择使得L最小的卷积核。

### Destruction ( Magnitude-based Pruning )
* 移除权重小于阈值的连接以及输出小于阈值的神经元。
* 移除的连接或者神经元对于L的影响很小，可以通过重新训练的方式来进行弥补。
* 该种方法有两种变体：
    > 对于微小的权重进行剪枝
    * 剪枝之前先进行归一化操作（Batch Normalize），根据归一化之后的值进行剪枝。
    * 主要目的是减少空间占用，同时减少计算能力的需求。
    * 权重剪枝是一个迭代的过程，每一轮只减去最小的一些权重，然后进行训练以恢复准确率。
    * 更多的信息可以参考之前的工作 [Pruning-on-CNN](https://github.com/Wind-Wing/Pruning-on-CNN)
    >
    > 部分区域卷积
    * pipeline ![alt tag](https://github.com/Wind-Wing/readme_images/blob/master/NeST_Algo3.png)
    *   C - all feature maps store in a matrix form
        
        N - Kernel numebrs

        P x Q - feature map size
    * 卷积层只占中大约5%的空间，但是带来了90%-95%的FLOP（浮点运算）开销。
    * 主要目的是减少FLOP开销。
    * pruning ratio r set as 1%
    * 允许每个卷积核通过对于图像加一个mask矩阵的方式选择感兴趣的卷积区域，从而减少FLOP开销。
    * mask矩阵对应特征图大小，通过mask矩阵记录特征图上的某个像素点是否要卷积生成从而判断对于相应的输入图区域是否要卷积。
    * 这一过程是通过修改某个卷积核与某个像素点之间的连接是否建立来完成的。
    * 卷积核对于图像感兴趣的区域也是通过一个迭代的过程建立的。每次修剪掉一些不感兴趣的区域后进行重新训练以恢复准确率。
    * 在LeNet5 on MNIST上，观察到了卷积核普遍对于图像中心区域比较感兴趣。

# 实验结果
1. 对于MNIST和ImageNet数据集，分别基于LeNet与AlexNet构造种子网络。
2. 种子网络的神经元个数、卷积核个数、连接个数都少于原来的网络。对于权重可以做随机初始化，但是要保证种子种所有的神经元都是连通的，不能存在孤岛现象。
3. 增长过程（Growth Phase）

    * 越小的种子增长后生成的网络越小，对应的修剪后的网络也越小。但是需要越长的增长时间开销。
    * 对于一个任务，存在一个最小的种子大小。如果小于这个大小，只会增加增长时间，而不会缩小最后生成的网络。
4. 剪枝过程（Pruning Phase）

    * 剪枝前的网络规模越大，剪枝后的压缩率越大。这是因为对于同一个任务，在剪枝前网络规模越大，存在网络冗余情况越严重。
    * 剪枝前网络规模越大，剪枝后的网络规模也越大。
5. NeST网络生成的网络比所有只进行剪枝的方法效果更好。这是由剪枝的基本限制决定的：剪枝前网络规模越大，剪枝后的网络规模也越大。因此，由于剪枝前的网络并不是最小的，所有的纯剪枝方法都会从剪枝前的网络中继承一定数量的次优性（网络冗余）。
6. 激活函数更换机制（activation function shift mechanism）
    * 由于ReLU在训练时存在“dying ReLU”问题：当一个ReLU神经元进入不激活状态后，由于梯度为0，会一直卡在未激活状态。
    * 在增长阶段（Growth Phase），可以采用Leaky ReLU来缓解“dying ReLU”问题，提高准确率。
    * 增长完成后，将所有的Leaky ReLU更换为ReLU，然后重新训练以保持准确率。
    * 在剪枝时，保持ReLU来利用ReLU存在0输出区域的特性，来减少FLOP开销。
    

# NeST与人脑学习过程的关系
1. 大脑中突触联系的数量在不同的人类年龄段上有所不同。在婴儿刚出生时，突触联系的数量增长很快，几个月后开始下降，随后逐渐保持稳定。神经网络在NeST中的学习过程非常接近于这一曲线。最初的种子神经网络简单而稀疏，就像婴儿出生时的大脑。在生长阶段，其中的连接和神经元数量因为外界信息而大量增长，这就像人类婴儿的大脑对外界刺激做出反应。而在修剪阶段它减少了突触连接的数量，摆脱了大量冗余，这与婴儿形成成熟大脑的过程是类似的。
2. 大脑中的大部分学习过程都是由神经元之间的突触重新连接引起的。人类大脑每天都会新增和清除大量（高达 40％）的突触连接。NeST 唤醒新的连接，从而在学习过程中有效地重连更多的神经元对。因此，它模仿了人类大脑中「重新连接学习」的机制。
3. 大脑中只有一小部分神经元在所有时间里都是活跃的，这种现象被称为稀疏神经元反应。这种机制允许人类大脑在超低功耗下运行（20W）。而全连接的神经网络在推理中存在大量无意义的神经元反应。为了解决这个问题，普林斯顿的研究者们在NeST 中加入了一个基于重要性的神经元/连接修剪算法来消除冗余，从而实现了稀疏性和紧凑性。这大大减少了存储和计算需求。
4. 参考资料：https://www.jianshu.com/p/fd81bfa6791f

# Need to learn
1. Hebbian理论与其具体数学表达形式，结合page5推导。
2. 在实现中，卷积层在卷积之前会把输入层的值与一个weight矩阵相乘么？即是否对于输入的图片有一个连接强度。
3. 卷积层的BP算法如何进行？
4. 内部协变位移( internal covariate shift )？
5. We use batch normalization instead of dropout in our implementation, since batch normalization can act as a regularizer and eliminate the need for dropout. dropout与BN的作用、操作、区别